"""
=================================================================================
GENERATE ADVERSARIAL PATCH - T·∫°o mi·∫øng d√°n ƒë·ªëi kh√°ng
=================================================================================
Code n√†y t·∫°o ra file adversarial_patch.png ƒë·ªÉ s·ª≠ d·ª•ng trong Physical Attack.

K·ªπ thu·∫≠t c·ªët l√µi: EOT (Expectation Over Transformation)
- Xoay (Rotation): Gi·∫£ l·∫≠p vi·ªác d√°n nghi√™ng
- Co gi√£n (Scale): Gi·∫£ l·∫≠p ng∆∞·ªùi ƒë·ª©ng xa/g·∫ßn  
- Nhi·ªÖu (Noise): Gi·∫£ l·∫≠p ch·∫•t l∆∞·ª£ng camera k√©m/√°nh s√°ng m√¥i tr∆∞·ªùng
- V·ªã tr√≠ ng·∫´u nhi√™n: Gi·∫£ l·∫≠p patch ·ªü c√°c v·ªã tr√≠ kh√°c nhau

Target: Bi·∫øn "Person" th√†nh "Toaster" (Class 859) ho·∫∑c class kh√°c

C√°ch ch·∫°y:
1. T·∫°o th∆∞ m·ª•c 'data/' v√† b·ªè v√†o 10-20 ·∫£nh ng∆∞·ªùi l√†m background
2. Ch·∫°y: python generate_patch.py
3. Output: adversarial_patch.png
=================================================================================
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import models, transforms
from PIL import Image, ImageFilter
import numpy as np
import matplotlib.pyplot as plt
import random
import os
import requests
from io import BytesIO
import warnings
warnings.filterwarnings('ignore')

# ==========================================
# 1. C·∫§U H√åNH (CONFIG)
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PATCH_SIZE = (100, 100)     # K√≠ch th∆∞·ªõc mi·∫øng d√°n (pixel)
IMG_SIZE = (224, 224)       # K√≠ch th∆∞·ªõc ƒë·∫ßu v√†o c·ªßa Model (ResNet/MobileNet d√πng 224)
EPOCHS = 500                # S·ªë v√≤ng l·∫∑p hu·∫•n luy·ªán
LEARNING_RATE = 0.05
TARGET_CLASS = 859          # 859 = "toaster" (l√≤ n∆∞·ªõng). M·ª•c ti√™u: Bi·∫øn ng∆∞·ªùi -> L√≤ n∆∞·ªõng
                            # M·ªôt s·ªë class th√∫ v·ªã kh√°c:
                            # 954 = banana, 508 = computer keyboard, 703 = park bench
                            # 281 = tabby cat, 207 = golden retriever

# EOT Configuration
EOT_ROTATION_RANGE = (-30, 30)       # Xoay t·ª´ -30 ƒë·∫øn 30 ƒë·ªô
EOT_SCALE_RANGE = (0.15, 0.4)        # Patch chi·∫øm 15-40% ·∫£nh
EOT_BRIGHTNESS_RANGE = (0.7, 1.3)    # ƒê·ªô s√°ng 70%-130%
EOT_NOISE_LEVEL = 0.05               # M·ª©c nhi·ªÖu Gaussian
EOT_SAMPLES_PER_ITER = 5             # S·ªë bi·∫øn th·ªÉ EOT m·ªói iteration

# Ensemble Models ƒë·ªÉ tƒÉng Transferability (H2)
USE_ENSEMBLE = True
ENSEMBLE_MODELS = ['mobilenet', 'resnet50']  # C√≥ th·ªÉ th√™m 'inception', 'vgg16'

print("=" * 60)
print("üéØ ADVERSARIAL PATCH GENERATOR")
print("=" * 60)
print(f"üñ•Ô∏è  Device: {DEVICE}")
print(f"üìê Patch size: {PATCH_SIZE}")
print(f"üéØ Target class: {TARGET_CLASS}")
print(f"üîÑ Epochs: {EPOCHS}")
print(f"üß¨ Ensemble: {ENSEMBLE_MODELS if USE_ENSEMBLE else 'Disabled'}")
print("=" * 60)

# ==========================================
# 2. LOAD MODELS (Ensemble cho Transferability - H2)
# ==========================================
print("\nüì¶ Loading models...")

models_dict = {}

def load_model(name):
    """Load model theo t√™n"""
    if name == 'mobilenet':
        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)
    elif name == 'resnet50':
        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
    elif name == 'inception':
        model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)
        model.aux_logits = False
    elif name == 'vgg16':
        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
    elif name == 'densenet':
        model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)
    else:
        model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)
    
    model = model.to(DEVICE)
    model.eval()
    return model

if USE_ENSEMBLE:
    for name in ENSEMBLE_MODELS:
        print(f"   Loading {name}...", end=" ")
        models_dict[name] = load_model(name)
        print("‚úÖ")
else:
    models_dict['mobilenet'] = load_model('mobilenet')
    print("   Loaded MobileNetV2 ‚úÖ")

# ==========================================
# 3. LOAD DATA (Background Images)
# ==========================================
print("\nüìÇ Loading background images...")

# Chu·∫©n h√≥a d·ªØ li·ªáu theo chu·∫©n ImageNet
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

preprocess = transforms.Compose([
    transforms.Resize(IMG_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
])

def load_background_images(data_dir='data'):
    """Load t·∫•t c·∫£ ·∫£nh t·ª´ th∆∞ m·ª•c data/"""
    images = []
    
    if os.path.exists(data_dir):
        for f in os.listdir(data_dir):
            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                img_path = os.path.join(data_dir, f)
                try:
                    img = Image.open(img_path).convert('RGB')
                    images.append(img)
                except:
                    pass
    
    if len(images) == 0:
        print("   ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ·∫£nh trong th∆∞ m·ª•c 'data/'")
        print("   üì• ƒêang t·∫£i ·∫£nh m·∫´u t·ª´ internet...")
        
        # T·∫£i m·ªôt s·ªë ·∫£nh m·∫´u t·ª´ internet
        sample_urls = [
            "https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=400",  # Person 1
            "https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=400",  # Person 2
            "https://images.unsplash.com/photo-1539571696357-5a69c17a67c6?w=400",  # Person 3
        ]
        
        for i, url in enumerate(sample_urls):
            try:
                response = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
                if response.status_code == 200:
                    img = Image.open(BytesIO(response.content)).convert('RGB')
                    images.append(img)
                    print(f"      ‚úÖ Loaded sample image {i+1}")
            except Exception as e:
                print(f"      ‚ùå Failed to load sample {i+1}: {e}")
        
        # N·∫øu v·∫´n kh√¥ng c√≥ ·∫£nh, t·∫°o ·∫£nh gi·∫£
        if len(images) == 0:
            print("   ‚ö†Ô∏è  T·∫°o ·∫£nh synthetic ƒë·ªÉ demo...")
            for i in range(5):
                # T·∫°o ·∫£nh gradient ng·∫´u nhi√™n
                arr = np.random.randint(50, 200, (224, 224, 3), dtype=np.uint8)
                images.append(Image.fromarray(arr))
    
    print(f"   ‚úÖ Loaded {len(images)} background images")
    return images

background_images = load_background_images()

# ==========================================
# 4. EOT FUNCTIONS (Expectation Over Transformation - H4)
# ==========================================

def apply_eot_transform(patch_tensor, bg_tensor):
    """
    √Åp d·ª•ng EOT: D√°n patch v√†o ·∫£nh n·ªÅn v·ªõi c√°c bi·∫øn ƒë·ªïi ng·∫´u nhi√™n
    ƒë·ªÉ gi·∫£ l·∫≠p th·∫ø gi·ªõi th·ª±c.
    
    Args:
        patch_tensor: Tensor c·ªßa patch (3, H, W)
        bg_tensor: Tensor c·ªßa ·∫£nh n·ªÅn (3, H, W)
    
    Returns:
        Tensor ·∫£nh ƒë√£ ƒë∆∞·ª£c d√°n patch v·ªõi bi·∫øn ƒë·ªïi EOT
    """
    # Clone ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng tensor g·ªëc
    adv_img = bg_tensor.clone()
    patch_aug = patch_tensor.clone()
    
    # 1. Random Rotation (Xoay mi·∫øng d√°n)
    angle = random.uniform(*EOT_ROTATION_RANGE)
    patch_aug = transforms.functional.rotate(
        patch_aug.unsqueeze(0), angle, 
        interpolation=transforms.InterpolationMode.BILINEAR
    ).squeeze(0)
    
    # 2. Random Scale (Co gi√£n)
    scale_factor = random.uniform(*EOT_SCALE_RANGE)
    new_h = int(IMG_SIZE[0] * scale_factor)
    new_w = int(IMG_SIZE[1] * scale_factor)
    patch_aug = F.interpolate(
        patch_aug.unsqueeze(0), size=(new_h, new_w), mode='bilinear', align_corners=False
    ).squeeze(0)
    
    # 3. Random Brightness (ƒê·ªô s√°ng)
    brightness = random.uniform(*EOT_BRIGHTNESS_RANGE)
    patch_aug = patch_aug * brightness
    
    # 4. Random Noise (Nhi·ªÖu)
    if EOT_NOISE_LEVEL > 0:
        noise = torch.randn_like(patch_aug) * EOT_NOISE_LEVEL
        patch_aug = patch_aug + noise
    
    # Clamp v·ªÅ kho·∫£ng h·ª£p l·ªá
    patch_aug = torch.clamp(patch_aug, -3, 3)
    
    # 5. Random Position (V·ªã tr√≠ d√°n ng·∫´u nhi√™n)
    max_x = IMG_SIZE[1] - new_w
    max_y = IMG_SIZE[0] - new_h
    if max_x > 0 and max_y > 0:
        x_pos = random.randint(0, max_x)
        y_pos = random.randint(0, max_y)
    else:
        x_pos, y_pos = 0, 0
    
    # 6. D√°n patch l√™n ·∫£nh n·ªÅn
    adv_img[:, y_pos:y_pos+new_h, x_pos:x_pos+new_w] = patch_aug
    
    return adv_img


def compute_tv_loss(patch):
    """
    Total Variation Loss - L√†m patch m∆∞·ª£t h∆°n (gi·∫£m nhi·ªÖu h·∫°t)
    ƒê√¢y l√† ph·∫ßn c·ªßa H3 (Semantic) - patch nh√¨n t·ª± nhi√™n h∆°n
    """
    tv_h = torch.sum(torch.abs(patch[:, 1:, :] - patch[:, :-1, :]))
    tv_w = torch.sum(torch.abs(patch[:, :, 1:] - patch[:, :, :-1]))
    return tv_h + tv_w


def compute_ensemble_loss(adv_img, target_class, models_dict):
    """
    T√≠nh loss cho ensemble attack (H2 - Transferability)
    Loss = T·ªïng loss c·ªßa t·∫•t c·∫£ c√°c model
    """
    total_loss = 0
    target_tensor = torch.tensor([target_class], device=DEVICE)
    
    for name, model in models_dict.items():
        # Resize n·∫øu c·∫ßn (Inception c·∫ßn 299x299)
        if name == 'inception':
            img = F.interpolate(adv_img.unsqueeze(0), size=(299, 299), mode='bilinear')
        else:
            img = adv_img.unsqueeze(0)
        
        output = model(img)
        
        # Targeted Attack: T·ªëi ƒëa h√≥a x√°c su·∫•t c·ªßa target class
        # Loss = -log(prob(target)) = CrossEntropy v·ªõi target
        loss = -F.cross_entropy(output, target_tensor)
        total_loss += loss
    
    return total_loss / len(models_dict)


# ==========================================
# 5. TRAINING LOOP
# ==========================================
print("\nüöÄ B·∫Øt ƒë·∫ßu t·∫°o Adversarial Patch...")
print(f"   Target: Class {TARGET_CLASS}")

# Kh·ªüi t·∫°o patch ng·∫´u nhi√™n
# Option 1: Random noise
# patch = torch.rand((3, PATCH_SIZE[0], PATCH_SIZE[1]), device=DEVICE, requires_grad=True)

# Option 2: Gradient-friendly initialization (th∆∞·ªùng converge nhanh h∆°n)
patch = torch.zeros((3, PATCH_SIZE[0], PATCH_SIZE[1]), device=DEVICE)
patch = patch + 0.5  # Start from gray
patch = patch + torch.randn_like(patch) * 0.1  # Add small noise
patch = patch.requires_grad_(True)

# Optimizer
optimizer = optim.Adam([patch], lr=LEARNING_RATE)

# Scheduler ƒë·ªÉ gi·∫£m learning rate
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)

# Training history
loss_history = []
best_loss = float('inf')
best_patch = None

# Labels ƒë·ªÉ hi·ªÉn th·ªã
try:
    url_labels = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"
    labels = requests.get(url_labels, timeout=5).json()
except:
    labels = [f"class_{i}" for i in range(1000)]

print(f"   üéØ Target label: {labels[TARGET_CLASS]}")
print("\n" + "-" * 50)

for epoch in range(EPOCHS):
    optimizer.zero_grad()
    total_epoch_loss = 0
    
    # Ch·ªçn ng·∫´u nhi√™n m·ªôt ·∫£nh background (Batch Training)
    bg_image = random.choice(background_images)
    bg_tensor = preprocess(bg_image).to(DEVICE)
    
    # EOT: Ch·∫°y nhi·ªÅu bi·∫øn th·ªÉ v√† l·∫•y trung b√¨nh loss
    for _ in range(EOT_SAMPLES_PER_ITER):
        # √Åp d·ª•ng EOT transform
        adv_image = apply_eot_transform(patch, bg_tensor)
        
        # T√≠nh loss t·ª´ ensemble
        attack_loss = compute_ensemble_loss(adv_image, TARGET_CLASS, models_dict)
        total_epoch_loss += attack_loss
    
    # Trung b√¨nh loss qua c√°c EOT samples
    avg_attack_loss = total_epoch_loss / EOT_SAMPLES_PER_ITER
    
    # Total Variation Loss (H3 - Semantic smoothness)
    tv_loss = compute_tv_loss(patch)
    
    # T·ªïng loss = Attack loss + TV regularization
    # TV weight nh·ªè ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng qu√° nhi·ªÅu ƒë·∫øn hi·ªáu qu·∫£ t·∫•n c√¥ng
    total_loss = avg_attack_loss + 0.001 * tv_loss
    
    # Backward & Update
    total_loss.backward()
    optimizer.step()
    scheduler.step()
    
    # Clamp patch v·ªÅ kho·∫£ng h·ª£p l·ªá [0, 1] tr∆∞·ªõc khi normalize
    # Trong kh√¥ng gian normalized c·ªßa ImageNet, gi√° tr·ªã c√≥ th·ªÉ t·ª´ -2 ƒë·∫øn 3
    with torch.no_grad():
        # Clamp trong kh√¥ng gian pixel (sau denormalize s·∫Ω n·∫±m trong [0,1])
        patch.data.clamp_(-2.5, 2.5)
    
    # L∆∞u best patch
    if total_loss.item() < best_loss:
        best_loss = total_loss.item()
        best_patch = patch.clone().detach()
    
    loss_history.append(total_loss.item())
    
    # Progress report
    if epoch % 25 == 0 or epoch == EPOCHS - 1:
        # Ki·ªÉm tra prediction hi·ªán t·∫°i
        with torch.no_grad():
            test_adv = apply_eot_transform(patch, bg_tensor)
            test_output = list(models_dict.values())[0](test_adv.unsqueeze(0))
            probs = F.softmax(test_output, dim=1)[0]
            pred_idx = probs.argmax().item()
            target_prob = probs[TARGET_CLASS].item()
        
        print(f"Epoch {epoch:4d}/{EPOCHS} | Loss: {total_loss.item():.4f} | "
              f"Target prob: {target_prob:.2%} | Pred: {labels[pred_idx][:15]}")

print("-" * 50)
print("‚úÖ Training ho√†n t·∫•t!")

# ==========================================
# 6. L∆ØU K·∫æT QU·∫¢
# ==========================================
print("\nüíæ ƒêang l∆∞u k·∫øt qu·∫£...")

def save_patch(patch_tensor, filename):
    """L∆∞u patch tensor th√†nh ·∫£nh PNG"""
    # Denormalize
    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1).to(patch_tensor.device)
    std = torch.tensor(IMAGENET_STD).view(3, 1, 1).to(patch_tensor.device)
    patch_img = patch_tensor * std + mean
    patch_img = torch.clamp(patch_img, 0, 1)
    
    # Convert to PIL
    patch_np = (patch_img.cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
    Image.fromarray(patch_np).save(filename)
    return patch_np

# L∆∞u patch t·ªët nh·∫•t
if best_patch is not None:
    patch_np = save_patch(best_patch, "adversarial_patch.png")
    print("   ‚úÖ Saved: adversarial_patch.png")
else:
    patch_np = save_patch(patch, "adversarial_patch.png")
    print("   ‚úÖ Saved: adversarial_patch.png")

# L∆∞u patch v·ªõi k√≠ch th∆∞·ªõc l·ªõn h∆°n ƒë·ªÉ in
large_patch = F.interpolate(
    (best_patch if best_patch is not None else patch).unsqueeze(0),
    size=(500, 500), mode='bilinear', align_corners=False
).squeeze(0)
save_patch(large_patch, "adversarial_patch_printable.png")
print("   ‚úÖ Saved: adversarial_patch_printable.png (500x500 for printing)")

# ==========================================
# 7. VISUALIZATION
# ==========================================
print("\nüìä Generating visualization...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# 1. Loss history
axes[0, 0].plot(loss_history)
axes[0, 0].set_title("Training Loss")
axes[0, 0].set_xlabel("Epoch")
axes[0, 0].set_ylabel("Loss")
axes[0, 0].grid(True)

# 2. Final patch
axes[0, 1].imshow(patch_np)
axes[0, 1].set_title(f"Adversarial Patch\nTarget: {labels[TARGET_CLASS]}")
axes[0, 1].axis('off')

# 3. Original image
bg_sample = random.choice(background_images)
bg_np = np.array(bg_sample.resize(IMG_SIZE))
axes[0, 2].imshow(bg_np)
axes[0, 2].set_title("Original Image")
axes[0, 2].axis('off')

# 4-6. Patched images with different EOT
bg_tensor = preprocess(bg_sample).to(DEVICE)
final_patch = best_patch if best_patch is not None else patch

for i, ax in enumerate(axes[1]):
    with torch.no_grad():
        adv_img = apply_eot_transform(final_patch, bg_tensor)
        
        # Denormalize for display
        mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1).to(DEVICE)
        std = torch.tensor(IMAGENET_STD).view(3, 1, 1).to(DEVICE)
        adv_display = adv_img * std + mean
        adv_display = torch.clamp(adv_display, 0, 1)
        
        # Get prediction
        output = list(models_dict.values())[0](adv_img.unsqueeze(0))
        probs = F.softmax(output, dim=1)[0]
        pred_idx = probs.argmax().item()
        pred_prob = probs[pred_idx].item()
        target_prob = probs[TARGET_CLASS].item()
    
    ax.imshow(adv_display.cpu().permute(1, 2, 0).numpy())
    ax.set_title(f"EOT Sample {i+1}\nPred: {labels[pred_idx][:12]} ({pred_prob:.1%})\n"
                    f"Target: {target_prob:.1%}")
    ax.axis('off')

plt.tight_layout()
plt.savefig("training_visualization.png", dpi=150)
print("   ‚úÖ Saved: training_visualization.png")

plt.show()

# ==========================================
# 8. FINAL EVALUATION
# ==========================================
print("\n" + "=" * 60)
print("üìà FINAL EVALUATION")
print("=" * 60)

# Test tr√™n nhi·ªÅu ·∫£nh
success_count = 0
total_tests = min(len(background_images), 10)

print(f"\nTesting on {total_tests} images...")

with torch.no_grad():
    for i in range(total_tests):
        bg = background_images[i]
        bg_tensor = preprocess(bg).to(DEVICE)
        
        # Original prediction
        orig_output = list(models_dict.values())[0](bg_tensor.unsqueeze(0))
        orig_pred = orig_output.argmax().item()
        
        # Patched prediction
        adv_img = apply_eot_transform(final_patch, bg_tensor)
        adv_output = list(models_dict.values())[0](adv_img.unsqueeze(0))
        adv_pred = adv_output.argmax().item()
        adv_probs = F.softmax(adv_output, dim=1)[0]
        target_prob = adv_probs[TARGET_CLASS].item()
        
        # Check success
        is_success = adv_pred == TARGET_CLASS or adv_pred != orig_pred
        if is_success:
            success_count += 1
        
        status = "‚úÖ" if is_success else "‚ùå"
        print(f"   Image {i+1}: {labels[orig_pred][:12]:12s} -> {labels[adv_pred][:12]:12s} "
                f"(Target: {target_prob:.1%}) {status}")

print(f"\nüéØ Attack Success Rate: {success_count}/{total_tests} ({success_count/total_tests:.1%})")
print("=" * 60)

print("""
üìã H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG:

1. File 'adversarial_patch.png' - Patch nh·ªè ƒë·ªÉ test digital
2. File 'adversarial_patch_printable.png' - Patch l·ªõn ƒë·ªÉ IN RA GI·∫§Y
3. Ch·∫°y 'test_attack_webcam.py' ƒë·ªÉ demo v·ªõi webcam

üí° ƒê·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£:
- TƒÉng EPOCHS l√™n 1000-2000
- Th√™m nhi·ªÅu ·∫£nh v√†o th∆∞ m·ª•c 'data/'
- Th√™m models v√†o ENSEMBLE_MODELS (nh∆∞ 'inception', 'vgg16')
- ƒêi·ªÅu ch·ªânh TARGET_CLASS theo m·ª•c ti√™u c·ªßa b·∫°n
""")
